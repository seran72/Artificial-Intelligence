{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-BeOChlfOfY"
   },
   "source": [
    "## Mini-Project 3:  Computer Vision using GPU and Transfer Learning\n",
    "\n",
    "\n",
    "#### CSC 180 Intelligent Systems (Fall 2021)\n",
    "\n",
    "#### Dr. Haiquan Chen, California State University, Sacramento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rENMVFFibA3H"
   },
   "outputs": [],
   "source": [
    "# Insert your name, your id, course title, assignment id, and due date here as comment \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LlFpkRqpfOfa"
   },
   "source": [
    "\n",
    "## Helpful Functions for Tensorflow (Little Gems)\n",
    "\n",
    "The following functions will be used with TensorFlow to help preprocess the data.  They allow you to build the feature vector for a neural network. \n",
    "\n",
    "* Predictors/Inputs \n",
    "    * Fill any missing inputs with the median for that column.  Use **missing_median**.\n",
    "    * Encode textual/categorical values with **encode_text_dummy**.\n",
    "    * Encode numeric values with **encode_numeric_zscore**.\n",
    "* Output\n",
    "    * Discard rows with missing outputs.\n",
    "    * Encode textual/categorical values with **encode_text_index**.\n",
    "    * Do not encode output numeric values.\n",
    "* Produce final feature vectors (x) and expected output (y) with **to_xy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FOddo2bbfOfb"
   },
   "outputs": [],
   "source": [
    "from collections.abc import Sequence\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "\n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name, x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "\n",
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name] - mean) / sd\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the median\n",
    "def missing_median(df, name):\n",
    "    med = df[name].median()\n",
    "    df[name] = df[name].fillna(med)\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the default\n",
    "def missing_default(df, name, default_value):\n",
    "    df[name] = df[name].fillna(default_value)\n",
    "\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column. \n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if isinstance(target_type, Sequence) else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df[result].values.astype(np.float32), dummies.values.astype(np.float32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df[result].values.astype(np.float32), df[target].values.astype(np.float32)\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "\n",
    "# Regression chart.\n",
    "def chart_regression(pred,y,sort=True):\n",
    "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
    "    if sort:\n",
    "        t.sort_values(by=['y'],inplace=True)\n",
    "    a = plt.plot(t['y'].tolist(),label='expected')\n",
    "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def remove_outliers(df, name, sd):\n",
    "    drop_rows = df.index[(np.abs(df[name] - df[name].mean()) >= (sd * df[name].std()))]\n",
    "    df.drop(drop_rows, axis=0, inplace=True)\n",
    "\n",
    "\n",
    "# Encode a column to a range between normalized_low and normalized_high.\n",
    "def encode_numeric_range(df, name, normalized_low=-1, normalized_high=1,\n",
    "                         data_low=None, data_high=None):\n",
    "    if data_low is None:\n",
    "        data_low = min(df[name])\n",
    "        data_high = max(df[name])\n",
    "\n",
    "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
    "               * (normalized_high - normalized_low) + normalized_low\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qU8FIYWRVXM1"
   },
   "source": [
    "## Switch and Verify GPU\n",
    "\n",
    "### To enable GPU backend for your notebook. Runtime->Change runtime type->Hardware Accelerator->GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "1s9pBa09U_zJ",
    "outputId": "183eb19f-9882-441f-c017-67362642126d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFpkSEEDbTag"
   },
   "source": [
    "### If the above code output '/device:GPU:0',  you have switched to GPU successfully and you are ready to go. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "Kv39g9StfOff"
   },
   "source": [
    "## Part I:   Image classification without transfer learning\n",
    "\n",
    "https://www.cs.toronto.edu/~kriz/cifar.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0495-SoMfOfg"
   },
   "outputs": [],
   "source": [
    "#  Load cifar-10 data and split it to training and test\n",
    "\n",
    "\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "# The data split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9DWyVv_efOfj"
   },
   "outputs": [],
   "source": [
    "# print out data shape\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "\n",
    "\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5_Z_IKevu2Ji"
   },
   "outputs": [],
   "source": [
    "# print out a random image in x_train as numpy array\n",
    "x_train[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cbcMvRhqvWAc"
   },
   "outputs": [],
   "source": [
    "# print it out as image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.imshow(x_train[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qMwUTdqp0rUJ"
   },
   "outputs": [],
   "source": [
    "# Convert y_train from 2D to 1D \n",
    "\n",
    "y_train = y_train.reshape(50000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ojvUNZgG0xVe"
   },
   "outputs": [],
   "source": [
    "y_train.shape\n",
    "\n",
    "# expected output: (50000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HcrolrznfOf2"
   },
   "outputs": [],
   "source": [
    "# Convert y_test from 2D to 1D \n",
    "\n",
    "y_test = y_test.reshape(10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iT3IYTNqfOf6"
   },
   "outputs": [],
   "source": [
    "y_test.shape\n",
    "\n",
    "# expected output: (10000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5-lWtPlHfOf9"
   },
   "outputs": [],
   "source": [
    "# Convert class vectors to one hot format\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DFzHf8QefOgC"
   },
   "outputs": [],
   "source": [
    "# Convert data from int to float and normalize it\n",
    "\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QXXjBVW1lDt1"
   },
   "source": [
    "###  Write your code in the cell below to create a CNN model which contains the following types of operations (layers):   \n",
    "\n",
    "- Conv2D\n",
    "- Activation\n",
    "- MaxPooling2D\n",
    "- Flatten\n",
    "- Dropout\n",
    "- Dense\n",
    "\n",
    "### (optional) You are also encouraged to create multiple models with different activiation functions, different numbers of neurons and layers for performance comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "grVEwQfQfOgE"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(16, (3, 3), activation='relu', strides=(1, 1), \n",
    "    padding='same', input_shape=input_shape))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', strides=(1, 1), \n",
    "    padding='same'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', strides=(1, 1), \n",
    "    padding='same'))\n",
    "model.add(MaxPool2D((2, 2)))\n",
    "model.add(Conv2D(16, (3, 3), activation='relu', strides=(1, 1), \n",
    "    padding='same'))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', strides=(1, 1), \n",
    "    padding='same'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', strides=(1, 1), \n",
    "    padding='same'))\n",
    "model.add(MaxPool2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', \n",
    "     optimizer='adam',\n",
    "     metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FekC4kgffOgG"
   },
   "outputs": [],
   "source": [
    "# Print model summary\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVNwHr_AmKJi"
   },
   "source": [
    "### Write your code in the cell below for compile, earlystopping and fit. Notice that you should use earlystopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QUxCqaaZfOgI"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test), callbacks=[es])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving/loading model\n",
    "model.save('CNN_CIFAR.h5')\n",
    "model = load_model('CNN_CIFAR.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xigd26XOoU-F"
   },
   "source": [
    "### Write your code in the cell below to print out the Precision, Recall,  F1 score, and classification_*report*\n",
    "\n",
    "### Include your findings in the project report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2_92T0bsfOgQ"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "103RRPUvtOie"
   },
   "source": [
    "### Write your code in the cell below to show 3-5 images in the test set as well as their true labels and their labels predicted by your model.\n",
    "\n",
    "### Include your findings in the project report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fFryw76VtOie"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgBnw9dRfOgS"
   },
   "source": [
    "\n",
    "\n",
    "## Part II:   CNN model with Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H9rvtqrsfOgT"
   },
   "outputs": [],
   "source": [
    "# We load data again.   The data split between train and test sets:\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "\n",
    "\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UGSxqVVypDiX"
   },
   "source": [
    "### Here we would like to use one pre-trained model called VGG16.   For more details on VGG16, please go to https://neurohive.io/en/popular-networks/vgg16/\n",
    "\n",
    "\n",
    "### VGG16 supports down to 48x48 images as an input. However, the resolution of our images is too low, which is (32, 32) so we need to increase the resolution.   This is called upsampling. \n",
    "\n",
    "\n",
    "\n",
    "### Find a way to do upsampling for each image to increase its resolution from 32x32 to 64x64. One option is to use the function resize(), which is provided by scikit-image library (https://scikit-image.org/)\n",
    "\n",
    "\n",
    "### Hints: \n",
    "\n",
    "#### (1) If you use resize() in scikit-image, that function also normalizes the input image so you may not want to normalize twice.\n",
    "\n",
    "#### Learn from the examples here:  https://scikit-image.org/docs/stable/auto_examples/transform/plot_rescale.html\n",
    "\n",
    "#### (2) Apply upsampling to x_train and x_test seperately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XWwBKsh-fOgX"
   },
   "outputs": [],
   "source": [
    "import skimage.transform\n",
    "\n",
    "new_x_train = []\n",
    "\n",
    "\n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# this process may take about a few minutes ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0s0-oaKDfOgf"
   },
   "outputs": [],
   "source": [
    "new_x_test = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# this process may take about a few minutes ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pPjFo2oyqlGO"
   },
   "source": [
    "### Write your code in the cell below to do the following:\n",
    "\n",
    "- First convert y_train and y_test from 2D to 1D by using reshape() function \n",
    "- Next apply one-hot encoding to y_train and y_test by using tf.keras.utils.to_categorical() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fQuVaf2pkRdr"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)\n",
    "\n",
    "input_shape = (X_train.shape[1], X_train.shape[2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w_pLuCNjqvbf"
   },
   "outputs": [],
   "source": [
    "# double check shape\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "\n",
    "# expected output:  (50000, 10)\n",
    "# expected output:  (10000, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-bzRNdjfOgy"
   },
   "source": [
    "###  Load the pre-trained VGG16 model.  Write your code in the cell below to add each layer in VGG16 (excluding the top layers) to your new model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4HLwE00VfOgz"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "\n",
    "vgg_model = VGG16(weights='imagenet', include_top=False, input_shape=(64, 64, 3))   #  first hidden layer\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "  \n",
    "# write your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.add(Conv2D(16, (3, 3), activation='relu', strides=(1, 1), \n",
    "    padding='same', input_shape=input_shape))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', strides=(1, 1), \n",
    "    padding='same'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', strides=(1, 1), \n",
    "    padding='same'))\n",
    "model.add(MaxPool2D((2, 2)))\n",
    "model.add(Conv2D(16, (3, 3), activation='relu', strides=(1, 1), \n",
    "    padding='same'))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', strides=(1, 1), \n",
    "    padding='same'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', strides=(1, 1), \n",
    "    padding='same'))\n",
    "model.add(MaxPool2D((2, 2)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print out the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ki8D32hrl3E"
   },
   "source": [
    "### Write your code in the cell below to freeze the weights in each layer in the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X_oqboGwfOg1"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YArrSsS4rt_t"
   },
   "source": [
    "###  Write your code in the cell below to add some \"Dense\" layers as top layers.\n",
    "\n",
    "- Donot forget the output layer\n",
    "- Choose the right activation fucntion for the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fphbz0FhfOg3"
   },
   "outputs": [],
   "source": [
    "model.add(Flatten())\n",
    "\n",
    "\n",
    "# Add some \"Dense\" layers here, including output layer\n",
    "\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDOG92fcr1m8"
   },
   "source": [
    "###  Write your code below for compile and fit. \n",
    "\n",
    "### Train your new model. \n",
    "\n",
    "### Notice that you should use earlystopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xWrWTc2JfOg6",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "     optimizer='adam',\n",
    "     metrics=['acc'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "\n",
    "\n",
    "# Model training\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test), callbacks=[es])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# since we use GPU, the training time for each epoch for the transferred model is about 60 seconds.  \n",
    "# Let it run for a few epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ps2CZ8Xwrz_a"
   },
   "source": [
    "### Write your code below to print out the Precision, Recall, F1 score, and classification_report\n",
    "\n",
    "### Include your findings in the project report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rMRuLfS3fOg8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Model evaluation\n",
    "plt.title('Accuracy Score')\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.show()\n",
    "\n",
    "plt.title('Loss Values')\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Predicting test data\n",
    "predictions = model.predict(X_test)\n",
    "predictions = one_hot_encoder.inverse_transform(predictions)\n",
    "\n",
    "y_test = one_hot_encoder.inverse_transform(y_test)\n",
    "\n",
    "\n",
    "# Creating confusion matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "sns.heatmap(cm, cbar=False, xticklabels=labels, yticklabels=labels, fmt='d', annot=True, cmap=plt.cm.Blues)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PkatwrTod_ii"
   },
   "source": [
    "### Write your code in the cell below to show 3-5 images in the test set as well as their true labels and their labels predicted by your model with transfer learning. \n",
    "\n",
    "### Include your findings in the project report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PoRepUWbd_ii"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Displaying test data with its actual and predicted label\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2])\n",
    "\n",
    "y_test = y_test.astype(int)\n",
    "predictions = predictions.astype(int)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=7, nrows=3, sharex=False,\n",
    "    sharey=True, figsize=(17, 8))\n",
    "index = 0\n",
    "for i in range(3):\n",
    "    for j in range(7):\n",
    "        axes[i,j].set_title('actual:' + labels[y_test[index][0]] + '\\n' \n",
    "                            + 'predicted:' + labels[predictions[index][0]])\n",
    "        axes[i,j].imshow(X_test[index], cmap='gray')\n",
    "        axes[i,j].get_xaxis().set_visible(False)\n",
    "        axes[i,j].get_yaxis().set_visible(False)\n",
    "        index += 1\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "CSC180_Transfer_learning_project_on_cifar_10_(canvas_version).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
